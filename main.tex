
\documentclass[sigconf, nonacm]{acmart}

\begin{document}
\title{RepEng Project:  Empirical Evaluation of JSON Schema Extraction from MongoDB Collections}

%% The "author" command and its associated commands are used to define the authors and their affiliations.

\author{Ali Haydar Özdağ}
\orcid{0000-0001-5109-3700}
\affiliation{%
  \institution{University of Passau}
  \city{Passau}
  \country{Germany}
}
\email{oezdag01@ads.uni-passau.de}


\maketitle

\vspace{.3cm}
\begingroup\small\noindent\raggedright\textbf{Artifact Availability:}\\
The source code, data, and/or other artifacts have been made available at \url{https://github.com/ahoirg/JSONSchemaExtractionTool}.
\endgroup


\section{Introduction}

In the evolving landscape of data science, the efficient handling of schema-less data formats like JSON is pivotal. The approach detailed in 'An Approach for Schema Extraction of JSON and Extended JSON Document Collections'~\cite{SchemaExtraction} offers an innovative solution to this challenge.

In this paper, I will empirically analyze the validity of the hypotheses that the proposed approach extracts JSON schemas with 100\% accuracy, and that the time consumed by the algorithm of the approach is minimal, with the majority of the duration being attributed to data reading. My research replicates and extends the original study~\cite{SchemaExtraction} by conducting two pivotal experiments. Firstly, I evaluate the correctness of the extracted JSON schemas. Secondly, I measure the time efficiency of the JSON extraction process. 

For those interested in replicating the study or exploring the methodology further, a Dockerfile is provided, ensuring that my experimental environment is accessible and reproducible.

\section{Core Structural Elements}

This research was conducted in a carefully designed experimental environment to validate the hypotheses introduced earlier. Emphasizing reproducibility and replicability, the setup was containerized using Docker, ensuring consistent conditions for this analysis and easing future replication or expansion efforts. The subsequent sections detail the experimental setup, datasets and results of the experiments.

\subsection{Experimental Setup And Datasets }

This section outlines the experimental setup and datasets~\cite{JsonData} used for hypothesis testing. The experiments were conducted in a Docker container based on debian:bullseye-slim with LTS support, equipped with 5 GB of RAM and 3 CPUs, to ensure a stable and consistent environment. JSON schemas are extracted from the datasets stored in the MongoDB collections.

The experiments tested the schema extraction process using four distinct datasets~\cite{JsonData} as shown in \autoref{tab:tab1}. The first dataset, Doc-1, was populated with dummy records to replace sensitive information such as names and grades, thus maintaining data privacy within the container. To further challenge the experiments, additional keys were introduced into the JSON files.

\begin{table}
  \caption{Input Json Documents}
  \label{tab:tab1}
  \begin{tabular}{cp{4cm}l}
    \toprule
    \small Document& \small Content& \small Total Json\\
    \midrule
    \small Doc-1& \small Base document& \small 350.000\\
    \small Doc-2& \small Doc-1 with changes in the data values.& \small 350.000\\
    \small Doc-3& \small Similar to Doc-1, but the keys are in different order.& \small 350.000\\
    \small Doc-4& \small Similar to Doc-3, but the number of array elements is different.& \small 350.000\\
  \bottomrule
\end{tabular}
\end{table}

\begin{table}
  \caption{Results For  Datasets}
  \label{tab:tab2}
  \begin{tabular}{c c l c c c c c}
    \toprule
    \small C Name & \small N JSON & \small RS & \small ROrd& \small TB& \small TT & \small TB/TT\% \\
    \midrule
    \small Doc-1 & \small 350.000 & \small 1 & \small 1 & \small 28.324 & \small 28.764 & \small 98.88\% \\
    \small Doc-2 & \small 350.000 & \small 1 & \small 1 & \small 28.389 & \small 28.786 & \small 98.61\% \\
    \small Doc-3 & \small 350.000 & \small 120 & \small 1 & \small 27.550 & \small 28.031 & \small 98.28\% \\
    \small Doc-4 & \small 350.000 & \small 120 & \small 1 & \small 27.717 & \small 28.141 & \small 98.49\% \\
    \bottomrule
  \end{tabular}
  \smallskip
\begin{tabular}{l}
  \tiny 
  C Name - Collection Name. RS - Raw schemas. ROrd - Raw schemas with ordered structure. \\
  \tiny  
  TB - Time to obtain the raw schemas. TT - Total time. TB/TT\% - Percentage of TB/TT.
\end{tabular}
\end{table}

\subsection{Results}
In the first experiment,the documents were subjected to JSON schema extraction, resulting in a single raw schema for Doc-1 and Doc-2 and 120 raw schemas for Doc-3 and Doc-4 due to unordered keys. After sorting alphabetically, same raw schemas with ordered structure are obtained for all documents. This uniformity across 1.4 million JSON validates the accuracy of the extraction method in schema identification. Also according to MD5 and SHA-256 checksum calculations, the four JSON files have the same hash values. This indicates that the contents of the files are the same in terms of bitwise identity. The first hypothesis, 100\% accuracy, was proven.

In the second experiment, I evaluated the process efficiency by examining the TB/TT ratio for each document. As shown in \autoref{tab:tab2}, the TB/TT ratio consistently greater than 98\% across all documents. This finding confirms the second hypothesis, which states that the bottleneck in processing time is in file reading, not in the approach's algorithm.
%\clearpage

\bibliographystyle{ACM-Reference-Format}
\bibliography{sample}


\end{document}
\endinput
